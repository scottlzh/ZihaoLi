---
layout: archive
title: "Research Experience"
permalink: /research/
author_profile: true
collection: portfolio
---

<!-- <hr color="000000"/> -->

{% include base_path %}

<hr color="#FFFFFF" />

# <i>Work Experience</i>

## <b>Research Intern in Project Silica</b>
* 2021,_Microsoft Research Cambridge, Cambridge, UK_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr Ioan Stefanovici & Dr Katja Hofmann </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>I am exploring explainable RL-based approaches to scheduling in the Silica glass library, towards a scheduler for production deployment. </li>
      </ul>
    </li>
  </ul>
  <br/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


# <i>Individual Project</i>

## <b>Graph Neural Networks for Decentralized Path Planning</b>
* 2020, _Prorok Lab, University of Cambridge, Cambridge, UK_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr.Amanda Prorok</li>
    <li><b>Abstract</b>: 
      <ul>
        <li>We propose a combined architecture, where we train a convolutional neural network (CNN) that extracts adequate features from local observations, and a graph neural network (GNN) to communicate these features among robots with the ultimate goal of learning a decentralized sequential action policy that yields efficient path plans for all robots.</li>
        <li>GNN implementation offers an efficient architecture that operates in a localized manner, whereby information is shared over a multi-hop communication network, through explicit communication with nearby neighbors only</li>
      </ul>
    </li>
    <li>[<u><a href="http://ifaamas.org/Proceedings/aamas2020/pdfs/p1901.pdf">AAMAS2020.pdf</a></u>],[<u><a href="https://arxiv.org/abs/1912.06095">IROS2020.pdf</a></u>]</li>
  </ul>
  <a href="https://youtu.be/AGDk2RozpMQ
  " target="_blank"><img src="/images/customized/GraphMAPF2020.png" 
  alt="IMAGE ALT TEXT HERE" width="560" height="315" border="10" /></a>
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Estimation of Tissue Oxygen Saturation based on Image to Image Translation</b>
* 2018, _The Hamlyn Centre, Imperial College London, London, UK_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Motivation</b>: Investigate a non-invasive intra-operative measurement of tissue oxygen saturation based on Hyperspectral Imaging.</li>
    <li><b>Supervisor</b>: Prof.Daniel S Elson </li>
    <li><b>Abstract</b>: 
      <ul>
        <li>The conditional Generative Adversarial Networks (cGAN) was used to develop pixel-level image-to-image translation approach, called RGB2StO2, to estimate tissue oxygen saturation (StO2) from RGB images directly. </li>
        <li>Dual-input network, called Dual2StO2, was developed to investigate the optimal setting of the fibre bundle to capture meaningful and informative images from HSI camera. </li>
      </ul>
    </li>
    <li>[<u><a href="http://qingbiaoli.github.io/files/HSMR2018.pdf">RGB2StO2.pdf</a></u>],[<u><a href="https://link.springer.com/content/pdf/10.1007%2Fs11548-019-01940-2.pdf">Dual2StO2.pdf</a></u>]</li>
  </ul>
  <br/>
  <img src='/images/customized/Dual2StO2.png' width="560" height="315"/>
  </fieldset>
</div>
<hr color="#FFFFFF" />



## <b>Academic Research Internship in Legged Robot</b>
* Summer 2017, _Intelligence Robot Lab, Zhejiang University_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr. Qiuguo Zhu </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Robust control of bipedal walking for legged robot. </li>
        <li>Carried out physical experiment. </li>
      </ul>
    </li>
  </ul>
  <br/>
    <img src='/images/customized/ZJU2.gif' />
  </fieldset>
</div>
<hr color="#FFFFFF" />

## <b>Research Assistant in Bipedal Walking of Humanoid Robot</b>
* 2017, _SLMC, School of Informatics, The University of Edinburgh_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr. Zhibin Li </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Research model-free control of bipedal walking for humanoid robotics. </li>
        <li>Theoretical proof and simulation validation of online parameter estimation to obtain robust control of bipedal walking.</li>
      </ul>
    </li>
    <li>[<u><a href="http://qingbiaoli.github.io/files/humanoid2017.pdf">Humanoid2017.pdf</a></u>]</li>
  </ul>
  <br/>
    <img src='/images/customized/Humanoid2017_demo.png' width="560" height="315"/>
    <img src='/images/customized/Humanoid_case2.gif' width="560" height="315"/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Missile Impact on Snow (MEng Thesis with Distinction)</b>
* 2015, _The University of Edinburgh_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr Filipe Teixeira-Dias </li>
    <li><b>Project description</b>: 
      <ul>
        <li>This study aimed to optimize the design of the impactor developed by British Antarctic survey for long-term tracking on the motion of the glaciers. </li>
        <li>Investigated the characteristics of the impact dynamics of the impactor and its interaction with different types of snow, covering a range of impact energies.  </li>
      </ul>
    </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>CAD modelling of the impactor. </li>
        <li>Signal processing of data from accelerometer, and analysed on the results.   </li>
      </ul>
    </li>
  </ul>
  <br/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Research Assistant in Industrial Robotics (Funded by Erasmus+)</b>
* 2015, _The Institute of Production Engineering and Machine Tools (IFW), Leibniz University of Hanover_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dipl.-Ing.Thomas Lepper </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Mechanism design for industrial robot for industrial-level milling process, includes CAD modelling transmission device and robot arm. </li>
        <li>Kinematic simulation to analyse torque distribution during operation. </li>
      </ul>
    </li>
  </ul>
  <br/>
  </fieldset>
</div>
<hr color="#FFFFFF" />


# <i>Collaborative Work</i>
<!-- --- -->
## <b>Real-time Surgical Environment Enhancement for Robot-Assisted Minimally Invasive Surgery</b>
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Project description</b>: 
      <ul>
        <li>We propose a  multi-scale Generative Adversarial Network (GAN)-based video super-resolution method to construct a framework for automatic zooming ratio adjustment. </li>
        <li>It can provide automatic real-time zooming for high-quality visualization of the Region Of Interest (ROI) during the surgical operation. </li>
        <li>The framework is validated with the JIGSAW dataset and Hamlyn Centre Laparoscopic/Endoscopic Video Datasets, with results demonstrating its practicability. </li>
      </ul>
    </li>
    <li>[<u><a href="https://arxiv.org/pdf/2011.04003.pdf">ICRA2021.pdf</a></u>]</li>
  </ul>
  <br/>
  <img src='/images/customized/SurgicalGAN2021.png' width="auto" height="auto"/>
  <ul>
  <li>Experiments of our pipeline in JIGASW dataset, demonstrating different phases of testing our pipeline in the suturing (a) and knot tying (b) tasks, while (c) summaries the correspondence of the predicted operations and the upscaling factors. </li>
  </ul>
  <!-- <img src='/images/customized/surface_reconsuction.gif' width="560" height="315"/> -->
  </fieldset>
</div>
<hr color="#FFFFFF" />


## <b>Vision-based Navigation in Flexible Endoscopy</b>
* 2017, _The Hamlyn Centre, Imperial College London_
<h4><a href="javascript:void(0)" class="dsphead" onclick="dsp(this)"><span class="dspchar">+</span> Detail</a></h4>
<div class="dspcont" style='display:none;'>
  <fieldset>
  <ul>
    <li><b>Supervisor</b>: Dr. George Mylonas </li>
    <li><b>Project description</b>: 
      <ul>
        <li>Simultaneously mapping the human colon and tracking the endoscope pose in real time during flexible endoscopy. </li>
      </ul>
    </li>
    <li><b>Duties included</b>: 
      <ul>
        <li>Investigated available visual SLAM methods (ORB-SLAM) and visual-inertial SLAM methods (VINS-Mono, OKVIS), and customize them for small scale, near focus. </li>
        <li>Our SLAM pipeline can obtain conclusive registration and surface reconstruction based on point cloud data. </li>
      </ul>
    </li>
  </ul>
  <br/>
  <img src='/images/customized/ORB_SLAM2_demo.gif' width="560" height="315"/>
  <img src='/images/customized/surface_reconsuction.gif' width="560" height="315"/>
  </fieldset>
</div>
<hr color="#FFFFFF" />



